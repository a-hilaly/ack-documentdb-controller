// Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License"). You may
// not use this file except in compliance with the License. A copy of the
// License is located at
//
//     http://aws.amazon.com/apache2.0/
//
// or in the "license" file accompanying this file. This file is distributed
// on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
// express or implied. See the License for the specific language governing
// permissions and limitations under the License.

// Code generated by ack-generate. DO NOT EDIT.

package db_cluster

import (
	"context"
	"errors"
	"fmt"
	"reflect"
	"strings"

	ackv1alpha1 "github.com/aws-controllers-k8s/runtime/apis/core/v1alpha1"
	ackcompare "github.com/aws-controllers-k8s/runtime/pkg/compare"
	ackcondition "github.com/aws-controllers-k8s/runtime/pkg/condition"
	ackerr "github.com/aws-controllers-k8s/runtime/pkg/errors"
	ackrequeue "github.com/aws-controllers-k8s/runtime/pkg/requeue"
	ackrtlog "github.com/aws-controllers-k8s/runtime/pkg/runtime/log"
	"github.com/aws/aws-sdk-go/aws"
	svcsdk "github.com/aws/aws-sdk-go/service/docdb"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"

	svcapitypes "github.com/aws-controllers-k8s/documentdb-controller/apis/v1alpha1"
)

// Hack to avoid import errors during build...
var (
	_ = &metav1.Time{}
	_ = strings.ToLower("")
	_ = &aws.JSONValue{}
	_ = &svcsdk.DocDB{}
	_ = &svcapitypes.DBCluster{}
	_ = ackv1alpha1.AWSAccountID("")
	_ = &ackerr.NotFound
	_ = &ackcondition.NotManagedMessage
	_ = &reflect.Value{}
	_ = fmt.Sprintf("")
	_ = &ackrequeue.NoRequeue{}
)

// sdkFind returns SDK-specific information about a supplied resource
func (rm *resourceManager) sdkFind(
	ctx context.Context,
	r *resource,
) (latest *resource, err error) {
	rlog := ackrtlog.FromContext(ctx)
	exit := rlog.Trace("rm.sdkFind")
	defer func() {
		exit(err)
	}()
	// If any required fields in the input shape are missing, AWS resource is
	// not created yet. Return NotFound here to indicate to callers that the
	// resource isn't yet created.
	if rm.requiredFieldsMissingFromReadManyInput(r) {
		return nil, ackerr.NotFound
	}

	input, err := rm.newListRequestPayload(r)
	if err != nil {
		return nil, err
	}
	var resp *svcsdk.DescribeDBClustersOutput
	resp, err = rm.sdkapi.DescribeDBClustersWithContext(ctx, input)
	rm.metrics.RecordAPICall("READ_MANY", "DescribeDBClusters", err)
	if err != nil {
		if awsErr, ok := ackerr.AWSError(err); ok && awsErr.Code() == "DBClusterNotFoundFault" {
			return nil, ackerr.NotFound
		}
		return nil, err
	}

	// Merge in the information we read from the API call above to the copy of
	// the original Kubernetes object we passed to the function
	ko := r.ko.DeepCopy()

	found := false
	for _, elem := range resp.DBClusters {
		if elem.AssociatedRoles != nil {
			f0 := []*svcapitypes.DBClusterRole{}
			for _, f0iter := range elem.AssociatedRoles {
				f0elem := &svcapitypes.DBClusterRole{}
				if f0iter.RoleArn != nil {
					f0elem.RoleARN = f0iter.RoleArn
				}
				if f0iter.Status != nil {
					f0elem.Status = f0iter.Status
				}
				f0 = append(f0, f0elem)
			}
			ko.Status.AssociatedRoles = f0
		} else {
			ko.Status.AssociatedRoles = nil
		}
		if elem.AvailabilityZones != nil {
			f1 := []*string{}
			for _, f1iter := range elem.AvailabilityZones {
				var f1elem string
				f1elem = *f1iter
				f1 = append(f1, &f1elem)
			}
			ko.Spec.AvailabilityZones = f1
		} else {
			ko.Spec.AvailabilityZones = nil
		}
		if elem.BackupRetentionPeriod != nil {
			ko.Spec.BackupRetentionPeriod = elem.BackupRetentionPeriod
		} else {
			ko.Spec.BackupRetentionPeriod = nil
		}
		if elem.CloneGroupId != nil {
			ko.Status.CloneGroupID = elem.CloneGroupId
		} else {
			ko.Status.CloneGroupID = nil
		}
		if elem.ClusterCreateTime != nil {
			ko.Status.ClusterCreateTime = &metav1.Time{*elem.ClusterCreateTime}
		} else {
			ko.Status.ClusterCreateTime = nil
		}
		if elem.DBClusterArn != nil {
			if ko.Status.ACKResourceMetadata == nil {
				ko.Status.ACKResourceMetadata = &ackv1alpha1.ResourceMetadata{}
			}
			tmpARN := ackv1alpha1.AWSResourceName(*elem.DBClusterArn)
			ko.Status.ACKResourceMetadata.ARN = &tmpARN
		}
		if elem.DBClusterIdentifier != nil {
			ko.Spec.DBClusterIdentifier = elem.DBClusterIdentifier
		} else {
			ko.Spec.DBClusterIdentifier = nil
		}
		if elem.DBClusterMembers != nil {
			f7 := []*svcapitypes.DBClusterMember{}
			for _, f7iter := range elem.DBClusterMembers {
				f7elem := &svcapitypes.DBClusterMember{}
				if f7iter.DBClusterParameterGroupStatus != nil {
					f7elem.DBClusterParameterGroupStatus = f7iter.DBClusterParameterGroupStatus
				}
				if f7iter.DBInstanceIdentifier != nil {
					f7elem.DBInstanceIdentifier = f7iter.DBInstanceIdentifier
				}
				if f7iter.IsClusterWriter != nil {
					f7elem.IsClusterWriter = f7iter.IsClusterWriter
				}
				if f7iter.PromotionTier != nil {
					f7elem.PromotionTier = f7iter.PromotionTier
				}
				f7 = append(f7, f7elem)
			}
			ko.Status.DBClusterMembers = f7
		} else {
			ko.Status.DBClusterMembers = nil
		}
		if elem.DBClusterParameterGroup != nil {
			ko.Status.DBClusterParameterGroup = elem.DBClusterParameterGroup
		} else {
			ko.Status.DBClusterParameterGroup = nil
		}
		if elem.DBSubnetGroup != nil {
			ko.Status.DBSubnetGroup = elem.DBSubnetGroup
		} else {
			ko.Status.DBSubnetGroup = nil
		}
		if elem.DbClusterResourceId != nil {
			ko.Status.DBClusterResourceID = elem.DbClusterResourceId
		} else {
			ko.Status.DBClusterResourceID = nil
		}
		if elem.DeletionProtection != nil {
			ko.Spec.DeletionProtection = elem.DeletionProtection
		} else {
			ko.Spec.DeletionProtection = nil
		}
		if elem.EarliestRestorableTime != nil {
			ko.Status.EarliestRestorableTime = &metav1.Time{*elem.EarliestRestorableTime}
		} else {
			ko.Status.EarliestRestorableTime = nil
		}
		if elem.EnabledCloudwatchLogsExports != nil {
			f13 := []*string{}
			for _, f13iter := range elem.EnabledCloudwatchLogsExports {
				var f13elem string
				f13elem = *f13iter
				f13 = append(f13, &f13elem)
			}
			ko.Status.EnabledCloudwatchLogsExports = f13
		} else {
			ko.Status.EnabledCloudwatchLogsExports = nil
		}
		if elem.Endpoint != nil {
			ko.Status.Endpoint = elem.Endpoint
		} else {
			ko.Status.Endpoint = nil
		}
		if elem.Engine != nil {
			ko.Spec.Engine = elem.Engine
		} else {
			ko.Spec.Engine = nil
		}
		if elem.EngineVersion != nil {
			ko.Spec.EngineVersion = elem.EngineVersion
		} else {
			ko.Spec.EngineVersion = nil
		}
		if elem.HostedZoneId != nil {
			ko.Status.HostedZoneID = elem.HostedZoneId
		} else {
			ko.Status.HostedZoneID = nil
		}
		if elem.KmsKeyId != nil {
			ko.Spec.KMSKeyID = elem.KmsKeyId
		} else {
			ko.Spec.KMSKeyID = nil
		}
		if elem.LatestRestorableTime != nil {
			ko.Status.LatestRestorableTime = &metav1.Time{*elem.LatestRestorableTime}
		} else {
			ko.Status.LatestRestorableTime = nil
		}
		if elem.MasterUsername != nil {
			ko.Spec.MasterUsername = elem.MasterUsername
		} else {
			ko.Spec.MasterUsername = nil
		}
		if elem.MultiAZ != nil {
			ko.Status.MultiAZ = elem.MultiAZ
		} else {
			ko.Status.MultiAZ = nil
		}
		if elem.PercentProgress != nil {
			ko.Status.PercentProgress = elem.PercentProgress
		} else {
			ko.Status.PercentProgress = nil
		}
		if elem.Port != nil {
			ko.Spec.Port = elem.Port
		} else {
			ko.Spec.Port = nil
		}
		if elem.PreferredBackupWindow != nil {
			ko.Spec.PreferredBackupWindow = elem.PreferredBackupWindow
		} else {
			ko.Spec.PreferredBackupWindow = nil
		}
		if elem.PreferredMaintenanceWindow != nil {
			ko.Spec.PreferredMaintenanceWindow = elem.PreferredMaintenanceWindow
		} else {
			ko.Spec.PreferredMaintenanceWindow = nil
		}
		if elem.ReadReplicaIdentifiers != nil {
			f26 := []*string{}
			for _, f26iter := range elem.ReadReplicaIdentifiers {
				var f26elem string
				f26elem = *f26iter
				f26 = append(f26, &f26elem)
			}
			ko.Status.ReadReplicaIdentifiers = f26
		} else {
			ko.Status.ReadReplicaIdentifiers = nil
		}
		if elem.ReaderEndpoint != nil {
			ko.Status.ReaderEndpoint = elem.ReaderEndpoint
		} else {
			ko.Status.ReaderEndpoint = nil
		}
		if elem.ReplicationSourceIdentifier != nil {
			ko.Status.ReplicationSourceIdentifier = elem.ReplicationSourceIdentifier
		} else {
			ko.Status.ReplicationSourceIdentifier = nil
		}
		if elem.Status != nil {
			ko.Status.Status = elem.Status
		} else {
			ko.Status.Status = nil
		}
		if elem.StorageEncrypted != nil {
			ko.Spec.StorageEncrypted = elem.StorageEncrypted
		} else {
			ko.Spec.StorageEncrypted = nil
		}
		if elem.StorageType != nil {
			ko.Spec.StorageType = elem.StorageType
		} else {
			ko.Spec.StorageType = nil
		}
		if elem.VpcSecurityGroups != nil {
			f32 := []*svcapitypes.VPCSecurityGroupMembership{}
			for _, f32iter := range elem.VpcSecurityGroups {
				f32elem := &svcapitypes.VPCSecurityGroupMembership{}
				if f32iter.Status != nil {
					f32elem.Status = f32iter.Status
				}
				if f32iter.VpcSecurityGroupId != nil {
					f32elem.VPCSecurityGroupID = f32iter.VpcSecurityGroupId
				}
				f32 = append(f32, f32elem)
			}
			ko.Status.VPCSecurityGroups = f32
		} else {
			ko.Status.VPCSecurityGroups = nil
		}
		found = true
		break
	}
	if !found {
		return nil, ackerr.NotFound
	}

	rm.setStatusDefaults(ko)
	if !clusterAvailable(&resource{ko}) {
		// Setting resource synced condition to false will trigger a requeue of
		// the resource. No need to return a requeue error here.
		ackcondition.SetSynced(&resource{ko}, corev1.ConditionFalse, nil, nil)
	} else {
		ackcondition.SetSynced(&resource{ko}, corev1.ConditionTrue, nil, nil)
	}
	return &resource{ko}, nil
}

// requiredFieldsMissingFromReadManyInput returns true if there are any fields
// for the ReadMany Input shape that are required but not present in the
// resource's Spec or Status
func (rm *resourceManager) requiredFieldsMissingFromReadManyInput(
	r *resource,
) bool {
	return false
}

// newListRequestPayload returns SDK-specific struct for the HTTP request
// payload of the List API call for the resource
func (rm *resourceManager) newListRequestPayload(
	r *resource,
) (*svcsdk.DescribeDBClustersInput, error) {
	res := &svcsdk.DescribeDBClustersInput{}

	if r.ko.Spec.DBClusterIdentifier != nil {
		res.SetDBClusterIdentifier(*r.ko.Spec.DBClusterIdentifier)
	}

	return res, nil
}

// sdkCreate creates the supplied resource in the backend AWS service API and
// returns a copy of the resource with resource fields (in both Spec and
// Status) filled in with values from the CREATE API operation's Output shape.
func (rm *resourceManager) sdkCreate(
	ctx context.Context,
	desired *resource,
) (created *resource, err error) {
	rlog := ackrtlog.FromContext(ctx)
	exit := rlog.Trace("rm.sdkCreate")
	defer func() {
		exit(err)
	}()
	input, err := rm.newCreateRequestPayload(ctx, desired)
	if err != nil {
		return nil, err
	}

	var resp *svcsdk.CreateDBClusterOutput
	_ = resp
	resp, err = rm.sdkapi.CreateDBClusterWithContext(ctx, input)
	rm.metrics.RecordAPICall("CREATE", "CreateDBCluster", err)
	if err != nil {
		return nil, err
	}
	// Merge in the information we read from the API call above to the copy of
	// the original Kubernetes object we passed to the function
	ko := desired.ko.DeepCopy()

	if resp.DBCluster.AssociatedRoles != nil {
		f0 := []*svcapitypes.DBClusterRole{}
		for _, f0iter := range resp.DBCluster.AssociatedRoles {
			f0elem := &svcapitypes.DBClusterRole{}
			if f0iter.RoleArn != nil {
				f0elem.RoleARN = f0iter.RoleArn
			}
			if f0iter.Status != nil {
				f0elem.Status = f0iter.Status
			}
			f0 = append(f0, f0elem)
		}
		ko.Status.AssociatedRoles = f0
	} else {
		ko.Status.AssociatedRoles = nil
	}
	if resp.DBCluster.AvailabilityZones != nil {
		f1 := []*string{}
		for _, f1iter := range resp.DBCluster.AvailabilityZones {
			var f1elem string
			f1elem = *f1iter
			f1 = append(f1, &f1elem)
		}
		ko.Spec.AvailabilityZones = f1
	} else {
		ko.Spec.AvailabilityZones = nil
	}
	if resp.DBCluster.BackupRetentionPeriod != nil {
		ko.Spec.BackupRetentionPeriod = resp.DBCluster.BackupRetentionPeriod
	} else {
		ko.Spec.BackupRetentionPeriod = nil
	}
	if resp.DBCluster.CloneGroupId != nil {
		ko.Status.CloneGroupID = resp.DBCluster.CloneGroupId
	} else {
		ko.Status.CloneGroupID = nil
	}
	if resp.DBCluster.ClusterCreateTime != nil {
		ko.Status.ClusterCreateTime = &metav1.Time{*resp.DBCluster.ClusterCreateTime}
	} else {
		ko.Status.ClusterCreateTime = nil
	}
	if ko.Status.ACKResourceMetadata == nil {
		ko.Status.ACKResourceMetadata = &ackv1alpha1.ResourceMetadata{}
	}
	if resp.DBCluster.DBClusterArn != nil {
		arn := ackv1alpha1.AWSResourceName(*resp.DBCluster.DBClusterArn)
		ko.Status.ACKResourceMetadata.ARN = &arn
	}
	if resp.DBCluster.DBClusterIdentifier != nil {
		ko.Spec.DBClusterIdentifier = resp.DBCluster.DBClusterIdentifier
	} else {
		ko.Spec.DBClusterIdentifier = nil
	}
	if resp.DBCluster.DBClusterMembers != nil {
		f7 := []*svcapitypes.DBClusterMember{}
		for _, f7iter := range resp.DBCluster.DBClusterMembers {
			f7elem := &svcapitypes.DBClusterMember{}
			if f7iter.DBClusterParameterGroupStatus != nil {
				f7elem.DBClusterParameterGroupStatus = f7iter.DBClusterParameterGroupStatus
			}
			if f7iter.DBInstanceIdentifier != nil {
				f7elem.DBInstanceIdentifier = f7iter.DBInstanceIdentifier
			}
			if f7iter.IsClusterWriter != nil {
				f7elem.IsClusterWriter = f7iter.IsClusterWriter
			}
			if f7iter.PromotionTier != nil {
				f7elem.PromotionTier = f7iter.PromotionTier
			}
			f7 = append(f7, f7elem)
		}
		ko.Status.DBClusterMembers = f7
	} else {
		ko.Status.DBClusterMembers = nil
	}
	if resp.DBCluster.DBClusterParameterGroup != nil {
		ko.Status.DBClusterParameterGroup = resp.DBCluster.DBClusterParameterGroup
	} else {
		ko.Status.DBClusterParameterGroup = nil
	}
	if resp.DBCluster.DBSubnetGroup != nil {
		ko.Status.DBSubnetGroup = resp.DBCluster.DBSubnetGroup
	} else {
		ko.Status.DBSubnetGroup = nil
	}
	if resp.DBCluster.DbClusterResourceId != nil {
		ko.Status.DBClusterResourceID = resp.DBCluster.DbClusterResourceId
	} else {
		ko.Status.DBClusterResourceID = nil
	}
	if resp.DBCluster.DeletionProtection != nil {
		ko.Spec.DeletionProtection = resp.DBCluster.DeletionProtection
	} else {
		ko.Spec.DeletionProtection = nil
	}
	if resp.DBCluster.EarliestRestorableTime != nil {
		ko.Status.EarliestRestorableTime = &metav1.Time{*resp.DBCluster.EarliestRestorableTime}
	} else {
		ko.Status.EarliestRestorableTime = nil
	}
	if resp.DBCluster.EnabledCloudwatchLogsExports != nil {
		f13 := []*string{}
		for _, f13iter := range resp.DBCluster.EnabledCloudwatchLogsExports {
			var f13elem string
			f13elem = *f13iter
			f13 = append(f13, &f13elem)
		}
		ko.Status.EnabledCloudwatchLogsExports = f13
	} else {
		ko.Status.EnabledCloudwatchLogsExports = nil
	}
	if resp.DBCluster.Endpoint != nil {
		ko.Status.Endpoint = resp.DBCluster.Endpoint
	} else {
		ko.Status.Endpoint = nil
	}
	if resp.DBCluster.Engine != nil {
		ko.Spec.Engine = resp.DBCluster.Engine
	} else {
		ko.Spec.Engine = nil
	}
	if resp.DBCluster.EngineVersion != nil {
		ko.Spec.EngineVersion = resp.DBCluster.EngineVersion
	} else {
		ko.Spec.EngineVersion = nil
	}
	if resp.DBCluster.HostedZoneId != nil {
		ko.Status.HostedZoneID = resp.DBCluster.HostedZoneId
	} else {
		ko.Status.HostedZoneID = nil
	}
	if resp.DBCluster.KmsKeyId != nil {
		ko.Spec.KMSKeyID = resp.DBCluster.KmsKeyId
	} else {
		ko.Spec.KMSKeyID = nil
	}
	if resp.DBCluster.LatestRestorableTime != nil {
		ko.Status.LatestRestorableTime = &metav1.Time{*resp.DBCluster.LatestRestorableTime}
	} else {
		ko.Status.LatestRestorableTime = nil
	}
	if resp.DBCluster.MasterUsername != nil {
		ko.Spec.MasterUsername = resp.DBCluster.MasterUsername
	} else {
		ko.Spec.MasterUsername = nil
	}
	if resp.DBCluster.MultiAZ != nil {
		ko.Status.MultiAZ = resp.DBCluster.MultiAZ
	} else {
		ko.Status.MultiAZ = nil
	}
	if resp.DBCluster.PercentProgress != nil {
		ko.Status.PercentProgress = resp.DBCluster.PercentProgress
	} else {
		ko.Status.PercentProgress = nil
	}
	if resp.DBCluster.Port != nil {
		ko.Spec.Port = resp.DBCluster.Port
	} else {
		ko.Spec.Port = nil
	}
	if resp.DBCluster.PreferredBackupWindow != nil {
		ko.Spec.PreferredBackupWindow = resp.DBCluster.PreferredBackupWindow
	} else {
		ko.Spec.PreferredBackupWindow = nil
	}
	if resp.DBCluster.PreferredMaintenanceWindow != nil {
		ko.Spec.PreferredMaintenanceWindow = resp.DBCluster.PreferredMaintenanceWindow
	} else {
		ko.Spec.PreferredMaintenanceWindow = nil
	}
	if resp.DBCluster.ReadReplicaIdentifiers != nil {
		f26 := []*string{}
		for _, f26iter := range resp.DBCluster.ReadReplicaIdentifiers {
			var f26elem string
			f26elem = *f26iter
			f26 = append(f26, &f26elem)
		}
		ko.Status.ReadReplicaIdentifiers = f26
	} else {
		ko.Status.ReadReplicaIdentifiers = nil
	}
	if resp.DBCluster.ReaderEndpoint != nil {
		ko.Status.ReaderEndpoint = resp.DBCluster.ReaderEndpoint
	} else {
		ko.Status.ReaderEndpoint = nil
	}
	if resp.DBCluster.ReplicationSourceIdentifier != nil {
		ko.Status.ReplicationSourceIdentifier = resp.DBCluster.ReplicationSourceIdentifier
	} else {
		ko.Status.ReplicationSourceIdentifier = nil
	}
	if resp.DBCluster.Status != nil {
		ko.Status.Status = resp.DBCluster.Status
	} else {
		ko.Status.Status = nil
	}
	if resp.DBCluster.StorageEncrypted != nil {
		ko.Spec.StorageEncrypted = resp.DBCluster.StorageEncrypted
	} else {
		ko.Spec.StorageEncrypted = nil
	}
	if resp.DBCluster.StorageType != nil {
		ko.Spec.StorageType = resp.DBCluster.StorageType
	} else {
		ko.Spec.StorageType = nil
	}
	if resp.DBCluster.VpcSecurityGroups != nil {
		f32 := []*svcapitypes.VPCSecurityGroupMembership{}
		for _, f32iter := range resp.DBCluster.VpcSecurityGroups {
			f32elem := &svcapitypes.VPCSecurityGroupMembership{}
			if f32iter.Status != nil {
				f32elem.Status = f32iter.Status
			}
			if f32iter.VpcSecurityGroupId != nil {
				f32elem.VPCSecurityGroupID = f32iter.VpcSecurityGroupId
			}
			f32 = append(f32, f32elem)
		}
		ko.Status.VPCSecurityGroups = f32
	} else {
		ko.Status.VPCSecurityGroups = nil
	}

	rm.setStatusDefaults(ko)
	// We expect the DB cluster to be in 'creating' status since we just
	// issued the call to create it, but I suppose it doesn't hurt to check
	// here.
	if clusterCreating(&resource{ko}) {
		// Setting resource synced condition to false will trigger a requeue of
		// the resource. No need to return a requeue error here.
		ackcondition.SetSynced(&resource{ko}, corev1.ConditionFalse, nil, nil)
		return &resource{ko}, nil
	}

	return &resource{ko}, nil
}

// newCreateRequestPayload returns an SDK-specific struct for the HTTP request
// payload of the Create API call for the resource
func (rm *resourceManager) newCreateRequestPayload(
	ctx context.Context,
	r *resource,
) (*svcsdk.CreateDBClusterInput, error) {
	res := &svcsdk.CreateDBClusterInput{}

	if r.ko.Spec.AvailabilityZones != nil {
		f0 := []*string{}
		for _, f0iter := range r.ko.Spec.AvailabilityZones {
			var f0elem string
			f0elem = *f0iter
			f0 = append(f0, &f0elem)
		}
		res.SetAvailabilityZones(f0)
	}
	if r.ko.Spec.BackupRetentionPeriod != nil {
		res.SetBackupRetentionPeriod(*r.ko.Spec.BackupRetentionPeriod)
	}
	if r.ko.Spec.DBClusterIdentifier != nil {
		res.SetDBClusterIdentifier(*r.ko.Spec.DBClusterIdentifier)
	}
	if r.ko.Spec.DBClusterParameterGroupName != nil {
		res.SetDBClusterParameterGroupName(*r.ko.Spec.DBClusterParameterGroupName)
	}
	if r.ko.Spec.DBSubnetGroupName != nil {
		res.SetDBSubnetGroupName(*r.ko.Spec.DBSubnetGroupName)
	}
	if r.ko.Spec.DeletionProtection != nil {
		res.SetDeletionProtection(*r.ko.Spec.DeletionProtection)
	}
	if r.ko.Spec.DestinationRegion != nil {
		res.SetDestinationRegion(*r.ko.Spec.DestinationRegion)
	}
	if r.ko.Spec.EnableCloudwatchLogsExports != nil {
		f7 := []*string{}
		for _, f7iter := range r.ko.Spec.EnableCloudwatchLogsExports {
			var f7elem string
			f7elem = *f7iter
			f7 = append(f7, &f7elem)
		}
		res.SetEnableCloudwatchLogsExports(f7)
	}
	if r.ko.Spec.Engine != nil {
		res.SetEngine(*r.ko.Spec.Engine)
	}
	if r.ko.Spec.EngineVersion != nil {
		res.SetEngineVersion(*r.ko.Spec.EngineVersion)
	}
	if r.ko.Spec.GlobalClusterIdentifier != nil {
		res.SetGlobalClusterIdentifier(*r.ko.Spec.GlobalClusterIdentifier)
	}
	if r.ko.Spec.KMSKeyID != nil {
		res.SetKmsKeyId(*r.ko.Spec.KMSKeyID)
	}
	if r.ko.Spec.MasterUserPassword != nil {
		tmpSecret, err := rm.rr.SecretValueFromReference(ctx, r.ko.Spec.MasterUserPassword)
		if err != nil {
			return nil, ackrequeue.Needed(err)
		}
		if tmpSecret != "" {
			res.SetMasterUserPassword(tmpSecret)
		}
	}
	if r.ko.Spec.MasterUsername != nil {
		res.SetMasterUsername(*r.ko.Spec.MasterUsername)
	}
	if r.ko.Spec.Port != nil {
		res.SetPort(*r.ko.Spec.Port)
	}
	if r.ko.Spec.PreSignedURL != nil {
		res.SetPreSignedUrl(*r.ko.Spec.PreSignedURL)
	}
	if r.ko.Spec.PreferredBackupWindow != nil {
		res.SetPreferredBackupWindow(*r.ko.Spec.PreferredBackupWindow)
	}
	if r.ko.Spec.PreferredMaintenanceWindow != nil {
		res.SetPreferredMaintenanceWindow(*r.ko.Spec.PreferredMaintenanceWindow)
	}
	if r.ko.Spec.SourceRegion != nil {
		res.SetSourceRegion(*r.ko.Spec.SourceRegion)
	}
	if r.ko.Spec.StorageEncrypted != nil {
		res.SetStorageEncrypted(*r.ko.Spec.StorageEncrypted)
	}
	if r.ko.Spec.StorageType != nil {
		res.SetStorageType(*r.ko.Spec.StorageType)
	}
	if r.ko.Spec.Tags != nil {
		f21 := []*svcsdk.Tag{}
		for _, f21iter := range r.ko.Spec.Tags {
			f21elem := &svcsdk.Tag{}
			if f21iter.Key != nil {
				f21elem.SetKey(*f21iter.Key)
			}
			if f21iter.Value != nil {
				f21elem.SetValue(*f21iter.Value)
			}
			f21 = append(f21, f21elem)
		}
		res.SetTags(f21)
	}
	if r.ko.Spec.VPCSecurityGroupIDs != nil {
		f22 := []*string{}
		for _, f22iter := range r.ko.Spec.VPCSecurityGroupIDs {
			var f22elem string
			f22elem = *f22iter
			f22 = append(f22, &f22elem)
		}
		res.SetVpcSecurityGroupIds(f22)
	}

	return res, nil
}

// sdkUpdate patches the supplied resource in the backend AWS service API and
// returns a new resource with updated fields.
func (rm *resourceManager) sdkUpdate(
	ctx context.Context,
	desired *resource,
	latest *resource,
	delta *ackcompare.Delta,
) (updated *resource, err error) {
	rlog := ackrtlog.FromContext(ctx)
	exit := rlog.Trace("rm.sdkUpdate")
	defer func() {
		exit(err)
	}()
	input, err := rm.newUpdateRequestPayload(ctx, desired, delta)
	if err != nil {
		return nil, err
	}

	var resp *svcsdk.ModifyDBClusterOutput
	_ = resp
	resp, err = rm.sdkapi.ModifyDBClusterWithContext(ctx, input)
	rm.metrics.RecordAPICall("UPDATE", "ModifyDBCluster", err)
	if err != nil {
		return nil, err
	}
	// Merge in the information we read from the API call above to the copy of
	// the original Kubernetes object we passed to the function
	ko := desired.ko.DeepCopy()

	if resp.DBCluster.AssociatedRoles != nil {
		f0 := []*svcapitypes.DBClusterRole{}
		for _, f0iter := range resp.DBCluster.AssociatedRoles {
			f0elem := &svcapitypes.DBClusterRole{}
			if f0iter.RoleArn != nil {
				f0elem.RoleARN = f0iter.RoleArn
			}
			if f0iter.Status != nil {
				f0elem.Status = f0iter.Status
			}
			f0 = append(f0, f0elem)
		}
		ko.Status.AssociatedRoles = f0
	} else {
		ko.Status.AssociatedRoles = nil
	}
	if resp.DBCluster.AvailabilityZones != nil {
		f1 := []*string{}
		for _, f1iter := range resp.DBCluster.AvailabilityZones {
			var f1elem string
			f1elem = *f1iter
			f1 = append(f1, &f1elem)
		}
		ko.Spec.AvailabilityZones = f1
	} else {
		ko.Spec.AvailabilityZones = nil
	}
	if resp.DBCluster.BackupRetentionPeriod != nil {
		ko.Spec.BackupRetentionPeriod = resp.DBCluster.BackupRetentionPeriod
	} else {
		ko.Spec.BackupRetentionPeriod = nil
	}
	if resp.DBCluster.CloneGroupId != nil {
		ko.Status.CloneGroupID = resp.DBCluster.CloneGroupId
	} else {
		ko.Status.CloneGroupID = nil
	}
	if resp.DBCluster.ClusterCreateTime != nil {
		ko.Status.ClusterCreateTime = &metav1.Time{*resp.DBCluster.ClusterCreateTime}
	} else {
		ko.Status.ClusterCreateTime = nil
	}
	if ko.Status.ACKResourceMetadata == nil {
		ko.Status.ACKResourceMetadata = &ackv1alpha1.ResourceMetadata{}
	}
	if resp.DBCluster.DBClusterArn != nil {
		arn := ackv1alpha1.AWSResourceName(*resp.DBCluster.DBClusterArn)
		ko.Status.ACKResourceMetadata.ARN = &arn
	}
	if resp.DBCluster.DBClusterIdentifier != nil {
		ko.Spec.DBClusterIdentifier = resp.DBCluster.DBClusterIdentifier
	} else {
		ko.Spec.DBClusterIdentifier = nil
	}
	if resp.DBCluster.DBClusterMembers != nil {
		f7 := []*svcapitypes.DBClusterMember{}
		for _, f7iter := range resp.DBCluster.DBClusterMembers {
			f7elem := &svcapitypes.DBClusterMember{}
			if f7iter.DBClusterParameterGroupStatus != nil {
				f7elem.DBClusterParameterGroupStatus = f7iter.DBClusterParameterGroupStatus
			}
			if f7iter.DBInstanceIdentifier != nil {
				f7elem.DBInstanceIdentifier = f7iter.DBInstanceIdentifier
			}
			if f7iter.IsClusterWriter != nil {
				f7elem.IsClusterWriter = f7iter.IsClusterWriter
			}
			if f7iter.PromotionTier != nil {
				f7elem.PromotionTier = f7iter.PromotionTier
			}
			f7 = append(f7, f7elem)
		}
		ko.Status.DBClusterMembers = f7
	} else {
		ko.Status.DBClusterMembers = nil
	}
	if resp.DBCluster.DBClusterParameterGroup != nil {
		ko.Status.DBClusterParameterGroup = resp.DBCluster.DBClusterParameterGroup
	} else {
		ko.Status.DBClusterParameterGroup = nil
	}
	if resp.DBCluster.DBSubnetGroup != nil {
		ko.Status.DBSubnetGroup = resp.DBCluster.DBSubnetGroup
	} else {
		ko.Status.DBSubnetGroup = nil
	}
	if resp.DBCluster.DbClusterResourceId != nil {
		ko.Status.DBClusterResourceID = resp.DBCluster.DbClusterResourceId
	} else {
		ko.Status.DBClusterResourceID = nil
	}
	if resp.DBCluster.DeletionProtection != nil {
		ko.Spec.DeletionProtection = resp.DBCluster.DeletionProtection
	} else {
		ko.Spec.DeletionProtection = nil
	}
	if resp.DBCluster.EarliestRestorableTime != nil {
		ko.Status.EarliestRestorableTime = &metav1.Time{*resp.DBCluster.EarliestRestorableTime}
	} else {
		ko.Status.EarliestRestorableTime = nil
	}
	if resp.DBCluster.EnabledCloudwatchLogsExports != nil {
		f13 := []*string{}
		for _, f13iter := range resp.DBCluster.EnabledCloudwatchLogsExports {
			var f13elem string
			f13elem = *f13iter
			f13 = append(f13, &f13elem)
		}
		ko.Status.EnabledCloudwatchLogsExports = f13
	} else {
		ko.Status.EnabledCloudwatchLogsExports = nil
	}
	if resp.DBCluster.Endpoint != nil {
		ko.Status.Endpoint = resp.DBCluster.Endpoint
	} else {
		ko.Status.Endpoint = nil
	}
	if resp.DBCluster.Engine != nil {
		ko.Spec.Engine = resp.DBCluster.Engine
	} else {
		ko.Spec.Engine = nil
	}
	if resp.DBCluster.EngineVersion != nil {
		ko.Spec.EngineVersion = resp.DBCluster.EngineVersion
	} else {
		ko.Spec.EngineVersion = nil
	}
	if resp.DBCluster.HostedZoneId != nil {
		ko.Status.HostedZoneID = resp.DBCluster.HostedZoneId
	} else {
		ko.Status.HostedZoneID = nil
	}
	if resp.DBCluster.KmsKeyId != nil {
		ko.Spec.KMSKeyID = resp.DBCluster.KmsKeyId
	} else {
		ko.Spec.KMSKeyID = nil
	}
	if resp.DBCluster.LatestRestorableTime != nil {
		ko.Status.LatestRestorableTime = &metav1.Time{*resp.DBCluster.LatestRestorableTime}
	} else {
		ko.Status.LatestRestorableTime = nil
	}
	if resp.DBCluster.MasterUsername != nil {
		ko.Spec.MasterUsername = resp.DBCluster.MasterUsername
	} else {
		ko.Spec.MasterUsername = nil
	}
	if resp.DBCluster.MultiAZ != nil {
		ko.Status.MultiAZ = resp.DBCluster.MultiAZ
	} else {
		ko.Status.MultiAZ = nil
	}
	if resp.DBCluster.PercentProgress != nil {
		ko.Status.PercentProgress = resp.DBCluster.PercentProgress
	} else {
		ko.Status.PercentProgress = nil
	}
	if resp.DBCluster.Port != nil {
		ko.Spec.Port = resp.DBCluster.Port
	} else {
		ko.Spec.Port = nil
	}
	if resp.DBCluster.PreferredBackupWindow != nil {
		ko.Spec.PreferredBackupWindow = resp.DBCluster.PreferredBackupWindow
	} else {
		ko.Spec.PreferredBackupWindow = nil
	}
	if resp.DBCluster.PreferredMaintenanceWindow != nil {
		ko.Spec.PreferredMaintenanceWindow = resp.DBCluster.PreferredMaintenanceWindow
	} else {
		ko.Spec.PreferredMaintenanceWindow = nil
	}
	if resp.DBCluster.ReadReplicaIdentifiers != nil {
		f26 := []*string{}
		for _, f26iter := range resp.DBCluster.ReadReplicaIdentifiers {
			var f26elem string
			f26elem = *f26iter
			f26 = append(f26, &f26elem)
		}
		ko.Status.ReadReplicaIdentifiers = f26
	} else {
		ko.Status.ReadReplicaIdentifiers = nil
	}
	if resp.DBCluster.ReaderEndpoint != nil {
		ko.Status.ReaderEndpoint = resp.DBCluster.ReaderEndpoint
	} else {
		ko.Status.ReaderEndpoint = nil
	}
	if resp.DBCluster.ReplicationSourceIdentifier != nil {
		ko.Status.ReplicationSourceIdentifier = resp.DBCluster.ReplicationSourceIdentifier
	} else {
		ko.Status.ReplicationSourceIdentifier = nil
	}
	if resp.DBCluster.Status != nil {
		ko.Status.Status = resp.DBCluster.Status
	} else {
		ko.Status.Status = nil
	}
	if resp.DBCluster.StorageEncrypted != nil {
		ko.Spec.StorageEncrypted = resp.DBCluster.StorageEncrypted
	} else {
		ko.Spec.StorageEncrypted = nil
	}
	if resp.DBCluster.StorageType != nil {
		ko.Spec.StorageType = resp.DBCluster.StorageType
	} else {
		ko.Spec.StorageType = nil
	}
	if resp.DBCluster.VpcSecurityGroups != nil {
		f32 := []*svcapitypes.VPCSecurityGroupMembership{}
		for _, f32iter := range resp.DBCluster.VpcSecurityGroups {
			f32elem := &svcapitypes.VPCSecurityGroupMembership{}
			if f32iter.Status != nil {
				f32elem.Status = f32iter.Status
			}
			if f32iter.VpcSecurityGroupId != nil {
				f32elem.VPCSecurityGroupID = f32iter.VpcSecurityGroupId
			}
			f32 = append(f32, f32elem)
		}
		ko.Status.VPCSecurityGroups = f32
	} else {
		ko.Status.VPCSecurityGroups = nil
	}

	rm.setStatusDefaults(ko)
	return &resource{ko}, nil
}

// newUpdateRequestPayload returns an SDK-specific struct for the HTTP request
// payload of the Update API call for the resource
func (rm *resourceManager) newUpdateRequestPayload(
	ctx context.Context,
	r *resource,
	delta *ackcompare.Delta,
) (*svcsdk.ModifyDBClusterInput, error) {
	res := &svcsdk.ModifyDBClusterInput{}

	if r.ko.Spec.BackupRetentionPeriod != nil {
		res.SetBackupRetentionPeriod(*r.ko.Spec.BackupRetentionPeriod)
	}
	if r.ko.Spec.DBClusterIdentifier != nil {
		res.SetDBClusterIdentifier(*r.ko.Spec.DBClusterIdentifier)
	}
	if r.ko.Spec.DBClusterParameterGroupName != nil {
		res.SetDBClusterParameterGroupName(*r.ko.Spec.DBClusterParameterGroupName)
	}
	if r.ko.Spec.DeletionProtection != nil {
		res.SetDeletionProtection(*r.ko.Spec.DeletionProtection)
	}
	if r.ko.Spec.EngineVersion != nil {
		res.SetEngineVersion(*r.ko.Spec.EngineVersion)
	}
	if r.ko.Spec.MasterUserPassword != nil {
		tmpSecret, err := rm.rr.SecretValueFromReference(ctx, r.ko.Spec.MasterUserPassword)
		if err != nil {
			return nil, ackrequeue.Needed(err)
		}
		if tmpSecret != "" {
			res.SetMasterUserPassword(tmpSecret)
		}
	}
	if r.ko.Spec.Port != nil {
		res.SetPort(*r.ko.Spec.Port)
	}
	if r.ko.Spec.PreferredBackupWindow != nil {
		res.SetPreferredBackupWindow(*r.ko.Spec.PreferredBackupWindow)
	}
	if r.ko.Spec.PreferredMaintenanceWindow != nil {
		res.SetPreferredMaintenanceWindow(*r.ko.Spec.PreferredMaintenanceWindow)
	}
	if r.ko.Spec.StorageType != nil {
		res.SetStorageType(*r.ko.Spec.StorageType)
	}
	if r.ko.Spec.VPCSecurityGroupIDs != nil {
		f14 := []*string{}
		for _, f14iter := range r.ko.Spec.VPCSecurityGroupIDs {
			var f14elem string
			f14elem = *f14iter
			f14 = append(f14, &f14elem)
		}
		res.SetVpcSecurityGroupIds(f14)
	}

	return res, nil
}

// sdkDelete deletes the supplied resource in the backend AWS service API
func (rm *resourceManager) sdkDelete(
	ctx context.Context,
	r *resource,
) (latest *resource, err error) {
	rlog := ackrtlog.FromContext(ctx)
	exit := rlog.Trace("rm.sdkDelete")
	defer func() {
		exit(err)
	}()
	if clusterDeleting(r) {
		return r, requeueWaitWhileDeleting
	}

	input, err := rm.newDeleteRequestPayload(r)
	if err != nil {
		return nil, err
	}
	var resp *svcsdk.DeleteDBClusterOutput
	_ = resp
	resp, err = rm.sdkapi.DeleteDBClusterWithContext(ctx, input)
	rm.metrics.RecordAPICall("DELETE", "DeleteDBCluster", err)
	return nil, err
}

// newDeleteRequestPayload returns an SDK-specific struct for the HTTP request
// payload of the Delete API call for the resource
func (rm *resourceManager) newDeleteRequestPayload(
	r *resource,
) (*svcsdk.DeleteDBClusterInput, error) {
	res := &svcsdk.DeleteDBClusterInput{}

	if r.ko.Spec.DBClusterIdentifier != nil {
		res.SetDBClusterIdentifier(*r.ko.Spec.DBClusterIdentifier)
	}
	res.SetSkipFinalSnapshot(true)

	return res, nil
}

// setStatusDefaults sets default properties into supplied custom resource
func (rm *resourceManager) setStatusDefaults(
	ko *svcapitypes.DBCluster,
) {
	if ko.Status.ACKResourceMetadata == nil {
		ko.Status.ACKResourceMetadata = &ackv1alpha1.ResourceMetadata{}
	}
	if ko.Status.ACKResourceMetadata.Region == nil {
		ko.Status.ACKResourceMetadata.Region = &rm.awsRegion
	}
	if ko.Status.ACKResourceMetadata.OwnerAccountID == nil {
		ko.Status.ACKResourceMetadata.OwnerAccountID = &rm.awsAccountID
	}
	if ko.Status.Conditions == nil {
		ko.Status.Conditions = []*ackv1alpha1.Condition{}
	}
}

// updateConditions returns updated resource, true; if conditions were updated
// else it returns nil, false
func (rm *resourceManager) updateConditions(
	r *resource,
	onSuccess bool,
	err error,
) (*resource, bool) {
	ko := r.ko.DeepCopy()
	rm.setStatusDefaults(ko)

	// Terminal condition
	var terminalCondition *ackv1alpha1.Condition = nil
	var recoverableCondition *ackv1alpha1.Condition = nil
	var syncCondition *ackv1alpha1.Condition = nil
	for _, condition := range ko.Status.Conditions {
		if condition.Type == ackv1alpha1.ConditionTypeTerminal {
			terminalCondition = condition
		}
		if condition.Type == ackv1alpha1.ConditionTypeRecoverable {
			recoverableCondition = condition
		}
		if condition.Type == ackv1alpha1.ConditionTypeResourceSynced {
			syncCondition = condition
		}
	}
	var termError *ackerr.TerminalError
	if rm.terminalAWSError(err) || err == ackerr.SecretTypeNotSupported || err == ackerr.SecretNotFound || errors.As(err, &termError) {
		if terminalCondition == nil {
			terminalCondition = &ackv1alpha1.Condition{
				Type: ackv1alpha1.ConditionTypeTerminal,
			}
			ko.Status.Conditions = append(ko.Status.Conditions, terminalCondition)
		}
		var errorMessage = ""
		if err == ackerr.SecretTypeNotSupported || err == ackerr.SecretNotFound || errors.As(err, &termError) {
			errorMessage = err.Error()
		} else {
			awsErr, _ := ackerr.AWSError(err)
			errorMessage = awsErr.Error()
		}
		terminalCondition.Status = corev1.ConditionTrue
		terminalCondition.Message = &errorMessage
	} else {
		// Clear the terminal condition if no longer present
		if terminalCondition != nil {
			terminalCondition.Status = corev1.ConditionFalse
			terminalCondition.Message = nil
		}
		// Handling Recoverable Conditions
		if err != nil {
			if recoverableCondition == nil {
				// Add a new Condition containing a non-terminal error
				recoverableCondition = &ackv1alpha1.Condition{
					Type: ackv1alpha1.ConditionTypeRecoverable,
				}
				ko.Status.Conditions = append(ko.Status.Conditions, recoverableCondition)
			}
			recoverableCondition.Status = corev1.ConditionTrue
			awsErr, _ := ackerr.AWSError(err)
			errorMessage := err.Error()
			if awsErr != nil {
				errorMessage = awsErr.Error()
			}
			recoverableCondition.Message = &errorMessage
		} else if recoverableCondition != nil {
			recoverableCondition.Status = corev1.ConditionFalse
			recoverableCondition.Message = nil
		}
	}
	// Required to avoid the "declared but not used" error in the default case
	_ = syncCondition
	if terminalCondition != nil || recoverableCondition != nil || syncCondition != nil {
		return &resource{ko}, true // updated
	}
	return nil, false // not updated
}

// terminalAWSError returns awserr, true; if the supplied error is an aws Error type
// and if the exception indicates that it is a Terminal exception
// 'Terminal' exception are specified in generator configuration
func (rm *resourceManager) terminalAWSError(err error) bool {
	if err == nil {
		return false
	}
	awsErr, ok := ackerr.AWSError(err)
	if !ok {
		return false
	}
	switch awsErr.Code() {
	case "DBClusterQuotaExceededFault",
		"DBSubnetGroupDoesNotCoverEnoughAZs",
		"InsufficientStorageClusterCapacity",
		"InvalidParameter",
		"InvalidParameterValue",
		"InvalidParameterCombination",
		"InvalidSubnet",
		"StorageQuotaExceeded":
		return true
	default:
		return false
	}
}
